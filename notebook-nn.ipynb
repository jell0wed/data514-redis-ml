{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redis as a high-performance machine learning database\n",
    "## NLP : Identifying toxic comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Research question:** can Redis serve as a real-time database for both training and testing purposes when training a neural network in production? How could we get Redis to act as the back-end database serving training examples when training our model.\n",
    "\n",
    "**IMPORTANT**: Make sure to have clone the repo, started a new Redis database instance locally before running this notebook. Refer to the `README.md` for further machine setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import redis\n",
    "from matplotlib import pyplot as plt\n",
    "import timeit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Neural Network implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis is meant to be used in multiple ways and is basically a large-scale, distributed and high-performance key-value datastore. It also supports indexing and storage of JSON document, which we'll be using here to store both our **training** and **testing** datasets. \n",
    "\n",
    "According to our architecture, the `train:*` index prefix contains all the training example in JSON format and the `test:*` contains all the testing examples.\n",
    "\n",
    "Let's make sure the database is filled up and ready to go by checking the dbsize(). \n",
    "\n",
    "**If the number printed below is zero, make sure you ran the `notebook-dataprep.ipynb` notebook and the `python ./fill_db.py` command. Prior to executing this notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to local redis database\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "# checking to make sure every entries was inserted into database\n",
    "r.dbsize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training and testing dataset into our Redis database, we'll define our neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make sure of the `torchtext` python package which provides an out-of-the-box sentence tokenizer as well as the GloVe embedding. For this project, we decided to use the GloVe 840B embeddings of 300 dimensions for each words. Below we initialize both the tokenizer and the glove embeddings, running this cell might take some as it needs to fetch the embeddings remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "global_vectors = GloVe(name='840B', dim=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write our own torch `Dataset` and `DataLoader` definition to be used in conjunction with Pytorch. More specifically, this dataset and dataloader are going to query the Redis database to return both our testing and training data in real-time.\n",
    "\n",
    "The dataset takes care of returning elements in our database one-by-one in an iterator fashion. \n",
    "The dataloader takes in the `collate_fn` as a parameter which `vectorize_batch` is passed in in order to encode the comment text into GloVe embeddings.\n",
    "\n",
    "This way, Redis can act as the backend database for training the neural network using real time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import sys\n",
    "\n",
    "MAX_COMMENT_LENGTH = 50\n",
    "EMBED_LEN = 300\n",
    "NORMAL_CLASS = \"normal\"\n",
    "TARGET_CLASSES = [NORMAL_CLASS, \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "test = 0\n",
    "def get_target_class(obj, idx):\n",
    "    obj[NORMAL_CLASS] = 0\n",
    "    labels = {attr: obj[attr] for attr in TARGET_CLASSES}\n",
    "    ret = next((k for k, v in labels.items() if v == 1), None)\n",
    "    return NORMAL_CLASS if ret is None else ret\n",
    "\n",
    "class RedisDataset(Dataset):\n",
    "    def __init__(self, redis, redis_key_prefix):\n",
    "        self.redis = redis\n",
    "        self.scan_iter = self.redis.scan_iter(redis_key_prefix + \"*\")\n",
    "        self.length = len(self.redis.keys(redis_key_prefix + \"*\"))\n",
    "        self.curr_idx = 0\n",
    "        self.redis_key_prefix = redis_key_prefix\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length - 1\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        key = self.redis_key_prefix + \"%d\" % idx\n",
    "        print(key, end='\\r')\n",
    "        obj = r.json().get(key)\n",
    "        if obj is None:\n",
    "            print(\"\", end='\\n')\n",
    "            raise IndexError()\n",
    "        label = get_target_class(obj, key)\n",
    "        return obj[\"comment_text\"], TARGET_CLASSES.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = RedisDataset(r, \"train:\"), RedisDataset(r, \"test:\")\n",
    "train_dataset, test_dataset = to_map_style_dataset(train_dataset), to_map_style_dataset(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We not create the pytorch DataLoader for our custom Redis-backed dataset. The `vectorize_batch` function takes care of tokenzing and embedding the `comment_text` into the GloVe embeddings up to a `MAX_LENGTH` and pads it with empty string to fit the length of 0 in the case where the comment text is too short. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch(batch):\n",
    "    X, Y = list(zip(*batch))\n",
    "    X = [tokenizer(x) for x in X]\n",
    "    X = [tokens+[\"\"] * (MAX_COMMENT_LENGTH-len(tokens)) if len(tokens) < MAX_COMMENT_LENGTH else tokens[:MAX_COMMENT_LENGTH] for tokens in X]\n",
    "    X_tensor = torch.zeros(len(batch), MAX_COMMENT_LENGTH, EMBED_LEN)\n",
    "    for i, tokens in enumerate(X):\n",
    "        X_tensor[i] = global_vectors.get_vecs_by_tokens(tokens)\n",
    "    return X_tensor.reshape(len(batch), -1), torch.tensor(Y)\n",
    "\n",
    "train_loader, test_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch), DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch)\n",
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define our `ToxicCommentClassifier` which is a simple Pytorch neural network. It consists of 4 linear layers of 256, 128, 64 and 7 output units. We use the ReLU activation function after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ToxicCommentClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToxicCommentClassifier, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(MAX_COMMENT_LENGTH*EMBED_LEN, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, len(TARGET_CLASSES))\n",
    "        )\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        return self.seq(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in val_loader:\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
    "        \n",
    "losses_graph = []\n",
    "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        for X, Y in tqdm(train_loader): # for each batch\n",
    "            Y_preds = model(X)\n",
    "\n",
    "            loss = loss_fn(Y_preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses_graph.append(torch.tensor(losses).mean())\n",
    "        if i%5==0:\n",
    "            print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "            CalcValLossAndAccuracy(model, loss_fn, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "embed_classifier = ToxicCommentClassifier()\n",
    "optimizer = Adam(embed_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "TrainModel(embed_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakePredictions(model, loader):\n",
    "    Y_shuffled, Y_preds = [], []\n",
    "    for X, Y in loader:\n",
    "        preds = model(X)\n",
    "        Y_preds.append(preds)\n",
    "        Y_shuffled.append(Y)\n",
    "    gc.collect()\n",
    "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
    "\n",
    "    return Y_shuffled.detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).detach().numpy()\n",
    "\n",
    "Y_actual, Y_preds = MakePredictions(embed_classifier, test_loader)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
    "print(\"\\nConfusion Matrix : \")\n",
    "print(confusion_matrix(Y_actual, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(model, text):\n",
    "    text_tokens = tokenizer(text)\n",
    "    text_tokens = text_tokens[:MAX_COMMENT_LENGTH]\n",
    "    X_tensor = torch.zeros(1, MAX_COMMENT_LENGTH, EMBED_LEN)\n",
    "    if len(text_tokens) < MAX_COMMENT_LENGTH:\n",
    "        text_tokens = text_tokens + ([\"\"] * (MAX_COMMENT_LENGTH - len(text_tokens)))\n",
    "    X_tensor[0] = global_vectors.get_vecs_by_tokens(text_tokens)\n",
    "    preds = model(X_tensor.reshape(1, -1))\n",
    "    \n",
    "    return TARGET_CLASSES[(preds.argmax(dim=-1).detach().numpy())[0]]\n",
    "\n",
    "print(\"Predicting sentence `this project was great`:\", do_prediction(embed_classifier, \"this project was great!\"))\n",
    "print(\"Predicting sentence `you are so bad you cant finish this project`:\", do_prediction(embed_classifier, \"you are so bad you cant finish this project\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses = list(map(lambda t: t.detach().item(), losses_graph))\n",
    "plt.plot(losses)\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the same `RedisDataset` to process unlabeled data in conjunction with Pytorch's DataLoader, similar to what we have done above for training and testing the model. Predictions are going to be stored at the same key for which it was returned.\n",
    "\n",
    "Each entry in the `unlabeled:` index will be retrieved, passed through the model, and moved to the `label:*` index with its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.commands.json.path import Path\n",
    "test_example_0 = {\n",
    "    'id': \"jepoisso-0\",\n",
    "    'comment_text': \"i love this project\",\n",
    "    'toxic': 0,\n",
    "    'severe_toxic': 0,\n",
    "    'obscene': 0,\n",
    "    'threat': 0,\n",
    "    'insult': 0,\n",
    "    'identity_hate': 0\n",
    "}\n",
    "r.json().set(\"unlabeled:0\", Path.root_path(), test_example_0)\n",
    "\n",
    "test_example_1 = {\n",
    "    'id': \"jepoisso-1\",\n",
    "    'comment_text': \"you are dumb\",\n",
    "    'toxic': 0,\n",
    "    'severe_toxic': 0,\n",
    "    'obscene': 0,\n",
    "    'threat': 0,\n",
    "    'insult': 0,\n",
    "    'identity_hate': 0\n",
    "}\n",
    "r.json().set(\"unlabeled:1\", Path.root_path(), test_example_1)\n",
    "\n",
    "from redis.commands.json.path import Path\n",
    "test_example_2 = {\n",
    "    'id': \"jepoisso-2\",\n",
    "    'comment_text': \"wow this is nice\",\n",
    "    'toxic': 0,\n",
    "    'severe_toxic': 0,\n",
    "    'obscene': 0,\n",
    "    'threat': 0,\n",
    "    'insult': 0,\n",
    "    'identity_hate': 0\n",
    "}\n",
    "r.json().set(\"unlabeled:2\", Path.root_path(), test_example_2)\n",
    "\n",
    "from redis.commands.json.path import Path\n",
    "test_example_3 = {\n",
    "    'id': \"jepoisso-3\",\n",
    "    'comment_text': \"this is great\",\n",
    "    'toxic': 0,\n",
    "    'severe_toxic': 0,\n",
    "    'obscene': 0,\n",
    "    'threat': 0,\n",
    "    'insult': 0,\n",
    "    'identity_hate': 0\n",
    "}\n",
    "r.json().set(\"unlabeled:3\", Path.root_path(), test_example_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_idx = len(r.keys(\"labeled:*\"))\n",
    "def process_unlabeled(model, loader):\n",
    "    for X, Y in loader:\n",
    "        preds = model(X)\n",
    "        predicted_labels = F.softmax(preds, dim=-1).argmax(dim=-1).detach().numpy()\n",
    "        print(\"Predicted labels = \", list(map(lambda i: TARGET_CLASSES[i], predicted_labels)))\n",
    "\n",
    "unlabeled_dataset = RedisDataset(r, \"unlabeled:\")\n",
    "unlabeled_dataset = to_map_style_dataset(unlabeled_dataset)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=1, collate_fn=vectorize_batch)\n",
    "\n",
    "process_unlabeled(embed_classifier, unlabeled_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
